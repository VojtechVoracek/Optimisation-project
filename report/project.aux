\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@nameuse{bbl@beforestart}
\citation{GD}
\citation{SGD}
\citation{COMPONENTFUNCTION}
\citation{SPHERE}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Motivation}{1}{section.1}\protected@file@percent }
\newlabel{sec:motivation}{{I}{1}{Motivation}{section.1}{}}
\newlabel{eq:finite-sum}{{1}{1}{Motivation}{equation.1.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Gradient descent}}{1}{algocf.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Stochastic gradient descent}}{1}{algocf.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Random reshuffling}}{1}{algocf.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Optimization problems}{1}{section.2}\protected@file@percent }
\citation{COMPONENTFUNCTION}
\citation{COMPONENTFUNCTION}
\citation{REGRESSION}
\citation{NEURALNETWORK}
\citation{COMPONENTFUNCTION}
\citation{COMPONENTFUNCTION}
\citation{DIABETES}
\citation{SKLEARN}
\citation{COMPONENTFUNCTION}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Sphere function}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}The component function}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Least squares linear regression}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-D}}Neural Network}{2}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Experiments}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{2}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Here we see, that RR outperforms SGD quite clearly. The results of RR vary little and stay equal for different dimensions, while SGD gets visibly worse as d gets larger. }}{3}{figure.1}\protected@file@percent }
\newlabel{fig:squareav1}{{1}{3}{Here we see, that RR outperforms SGD quite clearly. The results of RR vary little and stay equal for different dimensions, while SGD gets visibly worse as d gets larger}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Here we see, that on average RR leads to better results than SGD. In particular we see that SGD barely improves after a certain point, while RR is still improves after 2000 epochs.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:leastsquares1}{{2}{3}{Here we see, that on average RR leads to better results than SGD. In particular we see that SGD barely improves after a certain point, while RR is still improves after 2000 epochs}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Appendix}{3}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces For the component function we see that indeed RR outperforms SGD very quickly and quite significantly with over an order of magnitude. Again we see less variance between the different trainings.}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:component1}{{3}{3}{For the component function we see that indeed RR outperforms SGD very quickly and quite significantly with over an order of magnitude. Again we see less variance between the different trainings}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces In the neural network case we actually have that SGD performs better. While it is barely visible in this picture, the data shows, that initially RR improves quicker but at some point SGD takes over. }}{3}{figure.4}\protected@file@percent }
\newlabel{fig:nonconvex1}{{4}{3}{In the neural network case we actually have that SGD performs better. While it is barely visible in this picture, the data shows, that initially RR improves quicker but at some point SGD takes over}{figure.4}{}}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{GD}{1}
\bibcite{SGD}{2}
\bibcite{COMPONENTFUNCTION}{3}
\bibcite{SPHERE}{4}
\bibcite{REGRESSION}{5}
\bibcite{NEURALNETWORK}{6}
\bibcite{DIABETES}{7}
\bibcite{SKLEARN}{8}
\@writefile{toc}{\contentsline {section}{References}{4}{section*.1}\protected@file@percent }
